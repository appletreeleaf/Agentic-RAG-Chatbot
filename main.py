from loguru import logger
import streamlit as st
from utills import (print_message, get_session_history, StreamHandler,
                    get_filtered_relevant_docs)

from langchain import hub
from langchain_core.messages import ChatMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.document_loaders import PyPDFLoader, Docx2txtLoader, CSVLoader, TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_community.chat_message_histories import ChatMessageHistory

st.set_page_config(page_title="MyAssistant", page_icon="ğŸ¤—")
st.title("ğŸ¤— MyAssistant")

if "conversation_chain" not in st.session_state:
    st.session_state.conversation = None

# ì‚¬ì´ë“œ ë°”
with st.sidebar:
    session_id = st.text_input("Session ID", value="Chating Room")
    uploaded_file = st.file_uploader("Upload a file", type=['pdf', 'docx', 'csv', 'txt'], accept_multiple_files=True)

    if uploaded_file is not None:
        doc_list = []
        for doc in uploaded_file:
            file_name = doc.name
            with open(file_name, "wb") as file:
                file.write(doc.getvalue())
                logger.info(f"Uploaded {file_name}")
            try:
                if file_name.endswith('.pdf'):
                    loader = PyPDFLoader(file_name)
                elif file_name.endswith('.docx'):
                    loader = Docx2txtLoader(file_name)
                elif file_name.endswith('.csv'):
                    loader = CSVLoader(file_name)
                elif file_name.endswith('.txt'):
                    loader = TextLoader(file_name, encoding="utf-8")
                else:
                    st.error("Unsupported file type.")
                    continue
                
                documents = loader.load()
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)
                splitted_documents = text_splitter.split_documents(documents)
                doc_list.extend(splitted_documents)

                st.write("File has been uploaded!")
            except Exception as e:
                st.error(f"Error loading {file_name}: {e}")
                logger.error(f"Error loading {file_name}: {e}")

        # RAG ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™”
        if "vectorstore" not in st.session_state:
            if doc_list:
                try:
                    embeddings = OpenAIEmbeddings()
                    st.session_state["vectorstore"] = FAISS.from_documents(doc_list, embeddings)
                except Exception as e:
                    st.error(f"Error initializing vector store: {e}")
                    logger.error(f"Error initializing vector store: {e}")

        if "retriever" not in st.session_state:
            try:
                # ìœ ì‚¬ë„ ë†’ì€ K ê°œì˜ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
                k = 2

                # if not doc_list:
                #     raise ValueError("doc_list is empty. Make sure documents are loaded before initializing the retriever.")

                # (Sparse) bm25 retriever and (Dense) faiss retriever ë¥¼ ì´ˆê¸°í™” í•©ë‹ˆë‹¤.
                bm25_retriever = BM25Retriever.from_documents(doc_list)
                bm25_retriever.k = k

                faiss_vectorstore = FAISS.from_documents(doc_list, OpenAIEmbeddings())
                faiss_retriever = faiss_vectorstore.as_retriever(search_type="mmr", search_kwargs={"k": 5, "score_threshold": 0.8})

                # initialize the ensemble retriever
                ensemble_retriever = EnsembleRetriever(
                    retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5]
                )
                st.session_state["retriever"] = ensemble_retriever
            except Exception as e:
                st.error(f"Error initializing retriever: {e}")
                logger.error(f"Error initializing retriever: {e}")

    if st.button("Reset"):
        # st.session_state["message"] = []
        st.rerun()

# ë©”ì„¸ì§€ ë‚´ìš©ì„ ê¸°ë¡í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "message" not in st.session_state:
    st.session_state["message"] = [] 

# ì±„íŒ… ê¸°ë¡ì„ ì €ì¥í•˜ëŠ” ìƒíƒœ ë³€ìˆ˜
if "store" not in st.session_state:
    st.session_state["store"] = dict()

# ì±„íŒ… ê¸°ë¡ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì„ í•¨ìˆ˜í™”
print_message()

# Chat logic
if user_input := st.chat_input("ë©”ì„¸ì§€ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”"):
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ë‚´ìš©
    st.chat_message("user").write(f"{user_input}")
    st.session_state["message"].append(ChatMessage(role="user", content=user_input))

    # RAG
    if "retriever" in st.session_state:
        try:
            # Retriever ì„¤ì •
            retriever = st.session_state["vectorstore"].as_retriever(search_type="mmr", search_kwargs={"k": 5, "score_threshold": 0.8})
            relevant_docs = retriever.get_relevant_documents(user_input)

            # filtered_docs = get_filtered_relevant_docs(user_input, relevant_docs)
            if not relevant_docs:
                st.warning("No relevant documents found.")
                documents_text = ""
            else:
                # ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©ì„ ë¬¸ìì—´ë¡œ ë³‘í•©
                documents_text = "\n".join([doc.page_content for doc in relevant_docs])

            # LLM ì‘ë‹µ ìƒì„±
            llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
            rag_prompt = ChatPromptTemplate.from_messages(
                [
                    ("system",
                     """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. 
                     If you don't know the answer, just say that you don't know. 
                     Use three sentences maximum and keep the answer concise.
                     If you get a question that is not related to the document, please ignore the context and answer it.
                     Please answer in korean.
                     
                     #Context:
                     {context} 
                     
                     #Answer:
                     """
                     ),
                    MessagesPlaceholder(variable_name="history"),
                    ("human", "{question}"),
                    MessagesPlaceholder(variable_name='agent_scratchpad')
                ]
            )
            agent_prompt = hub.pull("hwchase17/openai-functions-agent")
            # agent ìƒì„±
            if "agent" not in st.session_state:
                search = TavilySearchResults(k=3)

                tool = create_retriever_tool(
                    retriever=retriever,  # í˜„ì¬ ì„¸ì…˜ì˜ retriever ì‚¬ìš©
                    name="search_documents",  # ë„êµ¬ ì´ë¦„
                    description="Searches and returns relevant excerpts from the uploaded documents."  # ë„êµ¬ ì„¤ëª…
                )
                tools = [search, tool]
                
                agent = create_openai_functions_agent(llm, tools, agent_prompt)
                agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
                st.session_state["agent_executor"] = agent_executor

            agent_executor = st.session_state["agent_executor"]
                # ì±„íŒ… ë©”ì‹œì§€ ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            message_history = ChatMessageHistory()

            # ì±„íŒ… ë©”ì‹œì§€ ê¸°ë¡ì´ ì¶”ê°€ëœ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            agent_with_chat_history = RunnableWithMessageHistory(
                agent_executor,
                # ëŒ€ë¶€ë¶„ì˜ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì„¸ì…˜ IDê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ì´ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ë‚´ ChatMessageHistoryë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
                # lambda session_id: message_history,
                get_session_history,
                # í”„ë¡¬í”„íŠ¸ì˜ ì§ˆë¬¸ì´ ì…ë ¥ë˜ëŠ” key: "input"
                input_messages_key="input",
                # í”„ë¡¬í”„íŠ¸ì˜ ë©”ì‹œì§€ê°€ ì…ë ¥ë˜ëŠ” key: "chat_history"
                history_messages_key="chat_history"
                )
            response = agent_with_chat_history.invoke(
                {
                    "input": user_input
                },
                # ì„¸ì…˜ IDë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
                # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ë‚´ ChatMessageHistoryë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì‹¤ì œë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤
                config={"configurable": {"session_id": "MyTestSessionID"}},
            )
            answer = response["output"]
            

            # # ì²´ì¸ ìƒì„±
            # chain = prompt | llm

            # chain_with_memory = RunnableWithMessageHistory(
            #     chain,
            #     get_session_history,
            #     input_messages_key="question",
            #     history_messages_key="history",
            # )

            # # ì‘ë‹µ ìƒì„±
            # response = chain_with_memory.invoke(
            #     {"context": documents_text, "question": user_input},
            #     config={"configurable": {"session_id": session_id}}
            # )
            # answer = response.content

            # AIì˜ ë‹µë³€ í‘œì‹œ
            with st.chat_message("assistant"):
                st.write(answer)

            # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
                with st.expander("ì°¸ê³  ë¬¸ì„œ"):
                    for doc in relevant_docs:
                        st.markdown(doc.metadata['source'], help=doc.page_content)

            st.session_state["message"].append(ChatMessage(role="assistant", content=answer))
        except Exception as e:
            st.error(f"Error during processing: {e}")
            logger.error(f"Error during processing: {e}")
    else:
        st.error("Retriever is not initialized.")